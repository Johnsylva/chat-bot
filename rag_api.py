import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from langfuse.openai import OpenAI  # Using Langfuse for tracing!
from pinecone import Pinecone
import re


load_dotenv()

# Initialize FastAPI
app = FastAPI()


# Configure CORS (So your frontend can talk to this API)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], #In production, specify your frontend URL
    allow_credentials = True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# Initialize clients
llm = OpenAI()
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
dense_index = pc.Index("flamehamster")  # Use YOUR Pinecone index name

# Store conversations - one per user
conversations = {}
conversation_chunks = {} #NEW: Track chunks per conversation - Instead of being an empty string - basically, we need to be able to determine what chunk is what and with a string where everything is one blob, that isn't possible so instead, we use a dictionary/hash where we can determine what chunk is what.


# Request body structure (like Rails strong params)
class ChatMessage(BaseModel):
    message: str
    conversation_id: str = "default"

# rag("What is GROSS", {})
def rag(user_input, rag_chunks):
    """Search Pinecone and ADD chunks to the dictionary"""
    results = dense_index.search(
        namespace="all-gross",
        query={
            "top_k": 3,
            "inputs": {"text": user_input}
        }
    )

      # RAG Step #2 - Convert chunks into one long string of documentation
    # documentation = ""
    for hit in results['result']['hits']:
        fields = hit.get('fields')
        chunk_text = fields.get('chunk_text')
    # rag_chunks = {}
        rag_chunks[hit['_id']] = chunk_text  # Store with ID as key
        # rag_chunks = {{flamehamster123: "Documentation for flamehamster"}}
        # rag_chunks = {{flamehamster123: "Documentation for flamehamster"}, {guineapigment123: "Documentation for guineapigment"}, {emrgency123: "Documentation for emrgency"}, {verbiage123: "Documentation for verbiage"}}
        # documentation += chunk_text

def system_prompt(rag_chunks=None):
    return {
        "role": "developer",
        "content": f"""
        <overview>
        You are an AI customer support technician who is
       knowledgeable about software products created by the company called GROSS.
       The products are:
       * Flamehamster, a web browser.
       * Rumblechirp, an email client.
       * GuineaPigment, a drawing tool for creating/editing SVGs
       * EMRgency, an electronic medical record system
       * Verbiage++, a content management system.


       You represent GROSS, and you are having a conversation with a human user
       who needs technical support with at least one of these GROSS products.

       When asking proactive follow-up
       questions, ask exactly one question at a time.
       When providing responses to question keep it short and to the point.

       </overview>
      
       You have access to certain excerpts of GROSS products' documentation
       that is pulled from a RAG system. Use this info (and no other info)
       to advise the user. Here are the documentation excerpts: 
       <documentation>{rag_chunks}</documentation>

       <instructions>
       * When helping troubleshoot a user's issue, ask a proactive question to
       help determine what exactly the issue is. When asking proactive follow-up
       questions, ask exactly one question at a time. 
       * When providing responses to question keep it short and to the point.
       * Do not mention the terms  'documentation excerpts" or "excerpts" in your response.
       </instructions>

       * Before you state any point other than a question, think
      carefully: which excerpt id does the advice come from? Use a special
      double-brackets notation before your advice to indicate the excerpt id
      that the advice comes from.


      For example:
      <example>
      [[flamehamster-chunk-30]]
      Since the Site Identity Button is gray and you are seeing "Your connection
      is not secure" on all sites, this indicates that Flamehamster is not able
      to establish secure (encrypted) connections. Normally, the Site Identity
      Button will be blue or green for secure sites, showing that the connection
      is encrypted and the site's identity is verified.
      </example>


      If you mention multiple points, use this notation BEFORE EACH POINT.
      For example:
      <example_response>
      [[flamehamster-chunk-7]]
      1. Make sure your Flamehamster security preferences have not been changed.
      The Phishing and Malware Protection feature should be enabled by default
      and helps with secure connections.


      [[flamehamster-chunk-8]]
      2. Check if your Flamehamster browser is up to date.
      Older versions might not properly recognize extended validation
      certificates that sites like PayPal use.
      </example_response>
       """
    }

def remove_bracket_tags(text):
   # Remove [[...]] and any immediate newlines following them
   return re.sub(r'\[\[.*?\]\]\s*(\r?\n)?', '', text)



# documentation = ""
# Root endpoint
@app.get("/")
def index():
    return {
        "message": "GROSS Support Chatbot API (with RAG)",
        # "endpoints": {
        #     "POST /chat": "Send a message (uses RAG)",
        #     "GET /conversations/{id}": "Get conversation history",
        #     "DELETE /conversations/{id}": "Clear conversation"
        # }
    }


# RAG Chat endpoint
@app.post("/chat")
def create(chat_message: ChatMessage):
    conversation_id = chat_message.conversation_id
    user_message = chat_message.message
    
    # Initialize conversation if new
    if conversation_id not in conversations:
        conversations[conversation_id] = [
            system_prompt(),
            {"role": "assistant", "content": 'How can I help you today?'}
        ]
    
        conversation_chunks[conversation_id] = {}
    # RAG Step #1 - Retrieve relevant chunks from Pinecone
    # results = dense_index.search(
    #     namespace="all-gross",
    #     query={
    #         "top_k": 3,
    #         "inputs": {"text": user_message}
    #     }
    # )
    rag(user_message, conversation_chunks[conversation_id])
    conversations[conversation_id][0] = system_prompt(conversation_chunks[conversation_id])
    
  
    
    # RAG Step #3 - Insert the retrieved documentation into the prompt
    # conversations[conversation_id].append({
    #     "role": "user",
    #     "content": f"""Here are excerpts from the official GROSS documentation: 
    #     {documentation}. Use whatever info from the above documentation excerpts 
    #     (and no other info) to answer the following query: {user_message}. 
    #     If the user asks something that you are unsure of, make sure to always 
    #     ask follow-up questions to make sure you're clear on what the user needs. 
    #     Also, if the user asks something that is vague and you're not sure what 
    #     service they're asking about, ask follow up questions."""
    # })

    # REWRITE

    # Add user message (just the message, no RAG in user prompt!)
    conversations[conversation_id].append({"role": "user", "content": user_message})
    
    # Get response from LLM
    response = llm.responses.create(
        model="gpt-4.1-mini",
        temperature=0.5,
        input=conversations[conversation_id]
    )
    
    assistant_message = response.output_text
    
    # Add response to history
    conversations[conversation_id].append({
        "role": "assistant",
        "content": assistant_message
    })
    
    # Return cleaned version (without citations) to the frontend!
    return {
        "message": remove_bracket_tags(assistant_message),
        "conversation_id": conversation_id
    }


# Get conversation history
@app.get("/conversations/{conversation_id}")
def show(conversation_id: str):
    if conversation_id not in conversations:
        return {"error": "Conversation not found"}
    
    return {
        "conversation_id": conversation_id,
        "history": conversations[conversation_id]
    }


# Clear conversation
@app.delete("/conversations/{conversation_id}")
def destroy(conversation_id: str):
    if conversation_id in conversations:
        del conversations[conversation_id]
        return {"message": "Conversation deleted"}
    
    return {"error": "Conversation not found"}